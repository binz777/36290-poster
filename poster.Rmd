---
title: "Predicting the Masses of Biggest Cluster Galaxies from their Optical Brightnesses and Shape"
author: "Bin, Megha, Athena"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: yes
  html_document:
    theme: spacelab
    toc: yes
    toc_float: yes
---

# Introduction 
Biggest Cluster Galaxies (BCGs) are large collections of galaxies that are studied greatly in astrophysics. More massive objects distort spacetime 
more and will have a larger effect on their environments. In this project, our goal is to determine the relationship between brightness and shapes of 
BCGs and their mass. Finding a direct relationship would allow us to skip intensive computations that have previously been used to predict mass.


# Data
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

rm(list=ls())
file.path = "https://raw.githubusercontent.com/pefreeman/36-290/master/PROJECT_DATASETS/SPIDERS_MASS/spiders_bcg_mass.Rdata"
load(url(file.path))
rm(file.path)
objects()

library(tidyverse)
library(MASS)
library(car)
library(rpart)
library(rpart.plot)
library(randomForest)
library(xgboost)
library(FNN)
library(corrplot)
library(leaps)
library(olsrr)
```

```{r, include = F}
# renaming variables

dff <- data.frame(predictors, response)

dff <- rename(dff, g_modS_CHI2NU = GAL_sdss_g_modS_CHI2NU	,
g_modS_C1_MAG	=	GAL_sdss_g_modS_C1_MAG	,
g_modS_C1_RE	=	GAL_sdss_g_modS_C1_RE	,
g_modS_C1_N	=	GAL_sdss_g_modS_C1_N	,
g_modS_C1_AR	=	GAL_sdss_g_modS_C1_AR	,
g_modS_C1_PA	=	GAL_sdss_g_modS_C1_PA	,
r_modS_CHI2NU	=	GAL_sdss_r_modS_CHI2NU	,
r_modS_C1_MAG	=	GAL_sdss_r_modS_C1_MAG	,
r_modS_C1_RE	=	GAL_sdss_r_modS_C1_RE	,
r_modS_C1_N	=	GAL_sdss_r_modS_C1_N	,
r_modS_C1_AR	=	GAL_sdss_r_modS_C1_AR	,
r_modS_C1_PA	=	GAL_sdss_r_modS_C1_PA	,
i_modS_CHI2NU	=	GAL_sdss_i_modS_CHI2NU	,
i_modS_C1_MAG	=	GAL_sdss_i_modS_C1_MAG	,
i_modS_C1_RE	=	GAL_sdss_i_modS_C1_RE	,
i_modS_C1_N	=	GAL_sdss_i_modS_C1_N	,
i_modS_C1_AR	=	GAL_sdss_i_modS_C1_AR	,
i_modS_C1_PA	=	GAL_sdss_i_modS_C1_PA	,
g_modV_CHI2NU	=	GAL_sdss_g_modV_CHI2NU	,
g_modV_C1_MAG	=	GAL_sdss_g_modV_C1_MAG	,
g_modV_C1_RE	=	GAL_sdss_g_modV_C1_RE	,
g_modV_C1_AR	=	GAL_sdss_g_modV_C1_AR	,
g_modV_C1_PA	=	GAL_sdss_g_modV_C1_PA	,
r_modV_CHI2NU	=	GAL_sdss_r_modV_CHI2NU	,
r_modV_C1_MAG	=	GAL_sdss_r_modV_C1_MAG	,
r_modV_C1_RE	=	GAL_sdss_r_modV_C1_RE	,
r_modV_C1_AR	=	GAL_sdss_r_modV_C1_AR	,
r_modV_C1_PA	=	GAL_sdss_r_modV_C1_PA	,
i_modV_CHI2NU	=	GAL_sdss_i_modV_CHI2NU	,
i_modV_C1_MAG	=	GAL_sdss_i_modV_C1_MAG	,
i_modV_C1_RE	=	GAL_sdss_i_modV_C1_RE	,
i_modV_C1_AR	=	GAL_sdss_i_modV_C1_AR	,
i_modV_C1_PA	=	GAL_sdss_i_modV_C1_PA	,
g_modSX_CHI2NU	=	GAL_sdss_g_modSX_CHI2NU	,
g_modSX_C1_MAG	=	GAL_sdss_g_modSX_C1_MAG	,
g_modSX_C1_RE	=	GAL_sdss_g_modSX_C1_RE	,
g_modSX_C1_N	=	GAL_sdss_g_modSX_C1_N	,
g_modSX_C1_AR	=	GAL_sdss_g_modSX_C1_AR	,
g_modSX_C1_PA	=	GAL_sdss_g_modSX_C1_PA	,
g_modSX_C2_MAG	=	GAL_sdss_g_modSX_C2_MAG	,
g_modSX_C2_RE	=	GAL_sdss_g_modSX_C2_RE	,
g_modSX_C2_AR	=	GAL_sdss_g_modSX_C2_AR	,
g_modSX_C2_PA	=	GAL_sdss_g_modSX_C2_PA	,
r_modSX_CHI2NU	=	GAL_sdss_r_modSX_CHI2NU	,
r_modSX_C1_MAG	=	GAL_sdss_r_modSX_C1_MAG	,
r_modSX_C1_RE	=	GAL_sdss_r_modSX_C1_RE	,
r_modSX_C1_N	=	GAL_sdss_r_modSX_C1_N	,
r_modSX_C1_AR	=	GAL_sdss_r_modSX_C1_AR	,
r_modSX_C1_PA	=	GAL_sdss_r_modSX_C1_PA	,
r_modSX_C2_MAG	=	GAL_sdss_r_modSX_C2_MAG	,
r_modSX_C2_RE	=	GAL_sdss_r_modSX_C2_RE	,
r_modSX_C2_AR	=	GAL_sdss_r_modSX_C2_AR	,
r_modSX_C2_PA	=	GAL_sdss_r_modSX_C2_PA	,
i_modSX_CHI2NU	=	GAL_sdss_i_modSX_CHI2NU	,
i_modSX_C1_MAG	=	GAL_sdss_i_modSX_C1_MAG	,
i_modSX_C1_RE	=	GAL_sdss_i_modSX_C1_RE	,
i_modSX_C1_N	=	GAL_sdss_i_modSX_C1_N	,
i_modSX_C1_AR	=	GAL_sdss_i_modSX_C1_AR	,
i_modSX_C1_PA	=	GAL_sdss_i_modSX_C1_PA	,
i_modSX_C2_MAG	=	GAL_sdss_i_modSX_C2_MAG	,
i_modSX_C2_RE	=	GAL_sdss_i_modSX_C2_RE	,
i_modSX_C2_AR	=	GAL_sdss_i_modSX_C2_AR	,
i_modSX_C2_PA	=	GAL_sdss_i_modSX_C2_PA	)
```
We will analyze data of 390 BCGs from Spectroscopic Identification of eROSITA Sources (SPIDERS), which contains 66 predictor variables including 
measurements of brightness and shape in different bands (g,r,i) and models (S, V, SX).  The response variable that we are interested in predicting is 
the log-base-10 mass of BCGs (logMass).

# EDA 


```{r, include = F}
# transformations
df <- dff[-c(68, 359), ]

log.chi <- df %>%
  dplyr::select(contains("_CHI")) %>%
  log()

log.re <- df %>%
  dplyr::select(contains("_RE")) %>%
  log()

df <- df %>%
  dplyr::select(-contains(c("_CHI", "_RE")))

df <- data.frame(df, log.chi, log.re)
predictors <- dplyr::select(df, -response)
response <- dplyr::select(df, response)
```
We performed log transformations on predictor variables related to CHI2NU and RE, which we found yielded better predictions and reduced skewness. 
We also found an overall pattern in the data where many of the predictors, when grouped by a specific property (e.g. MAG) across different models 
and wavelengths were all heavily linearly correlated. This suggests that many of the predictor variables may contain largely redundant information.

```{r}
pr.out = prcomp(predictors, scale=TRUE)

pr.var = data.frame(varExp = pr.out$sdev^2)
pve= pr.var/sum(pr.var$varExp)
pr.var = pr.var %>% mutate(pve = varExp / sum(varExp))

pve.df <- data.frame("PC" = 1:66, "pve" = pr.var$pve)

ggplot(pve.df, mapping=aes(x=PC, y=pve)) +
  geom_point() +
  theme(text = element_text(size=7),
        axis.text.x = element_text(angle=90)) +
  xlab("Principal Component ") +
  ylab("Proportion of Variance Explained")
```
From PCA, we found that the dimensions of our data can be reduced from 66 to around 35 or less, and still effectively explain over 95% of the 
dataâ€™s variance.

# Regression Analysis

```{r}
set.seed(101)
test.indices = sample(390, 78)
pred.test = predictors[test.indices,]
pred.train = predictors[-test.indices,]
resp.test = response[test.indices]
resp.train = response[-test.indices]
```

```{r}
THRESHOLD = 10
pred.vif = predictors
istop = 0
while ( istop == 0 ) {
  lm.out.update = lm(response~.,data=pred.vif)
  v = vif(lm.out.update)
  if ( max(v) > THRESHOLD ) {
    pred.vif = pred.vif[,-which.max(v)]
  } 
  else {
    istop = 1
  }
}
print(v)
```

```{r}
#LINEAR REGRESSION
#creating data frames based on multicollinearity test 
pred.vif.test = pred.vif[test.indices,]
pred.vif.train = pred.vif[-test.indices,]
resp.vif.test = response[test.indices]
resp.vif.train = response[-test.indices]

lm.mass.vif = lm(resp.vif.train ~ ., data=pred.vif.train)
summary(lm.mass.vif)

#linear regression residual plot
lm.resid_vif= resid(lm.mass.update)
plot(lm.resid_vif[0:350], ylab = "Residuals", xlab= "Mass") + abline(0, 0)

#MSE for linear regression
lm.mass.predict1 = predict(lm.mass.vif, newdata = pred.vif.test)
MSE.lin.vif = mean((resp.vif.test - lm.mass.predict1) ^ 2,na.rm = TRUE)
MSE.lin.vif
```

```{r}
#LASSO REGRESSION - accounting for collinear variables
#Data Frames as Matrices
model.train.vif = model.matrix(resp.vif.train ~ ., data=pred.vif.train)[,-1]
model.test.vif  = model.matrix(resp.vif.test ~ .,  data=pred.vif.test)[,-1]

#Lasso regression on the training data
lasso.mod.train.vif = glmnet(model.train.vif,resp.vif.train,alpha=1)

#determining the best value for lambda
set.seed(101)
cv.out.vif = cv.glmnet(model.train.vif,resp.vif.train,alpha=1)
bestlam.vif=cv.out.vif$lambda.min

#compute test-set MSE
lasso.pred.vif = predict(lasso.mod.train.vif,s=bestlam.vif,newx=model.test.vif)
MSE.lasso.vif = mean((lasso.pred.vif-resp.vif.test)^2)
MSE.lasso.vif
plot(lasso.mod.train.vif, xvar="lambda") + abline(v=log(bestlam.vif))
```

```{r}
#STEPWISE SELECTION (BACKWARD) - updated based on collinear variables
backward.selection.vif <- ols_step_backward_p(lm.mass.vif)
backward.selection.vif

#Backward MSE calculation
backward.selection.pred.vif = predict(backward.selection.vif$model,newdata=pred.vif.test)
MSE_backward_update= mean((resp.vif.test - backward.selection.pred.vif) ^ 2,na.rm = TRUE)
```

# Machine Learning Models
```{r}
#RANDOM FOREST
rf.out = randomForest(resp.train~.,data=pred.train, importance=TRUE)
forest.pred = predict(rf.out,newdata=pred.test,type="response")
MSE.forest = mean((resp.test - forest.pred) ^ 2,na.rm = TRUE)

varImp <- data.frame(rf.out$importance)
varImp <- varImp %>% mutate(predictors = row.names(.)) %>% arrange(desc(X.IncMSE))
varImp[1:5, c(3, 1, 2)]
```

```{r}
#kNN
#scale the train/test sets 
pred.train.scaled <- scale(pred.train)
pred.test.scaled <- scale(pred.test)
#determine optimal value of k and MSE
k.max = 50
mse.k <- rep(NA,k.max) 
for (i in 1:k.max){
  knnreg = knn.reg(pred.train.scaled, y=resp.train, k = i, algorithm="brute")
  knnmse = suppressWarnings(mean((resp.train - knnreg$pred)^2))
  mse.k[i] = knnmse
}
optimal.mse = which.min(mse.k)

#compare linear regression MSE to KKN MSE 
MSE.KNN = mse.k[optimal.mse]
```

# Final Analysis and Conclusion

```{r}
new.df <- data.frame(lasso.pred.vif, resp.vif.test)
ggplot(data = new.df, aes(x=resp.vif.test, y=lasso.pred.update)) + geom_point()+ geom_abline() + ylab("Predicted log(Mass)") + xlab("Observed log(Mass)") + xlim(10, 12.5) + ylim(10,12.5)
```
Overall, we identified a linear relationship between the brightness and shape measurements of BCGs and its log(Mass). However, the relationship 
was very weak and even our best model (lasso regression) did not effectively explain the data. Further analysis with more flexible nonlinear models 
performed at around the same level or worse in terms of prediction ability. Our best ML model, random forest, determined that redshift is by far the 
most important variables in predicting BCG mass. For the future, it may be beneficial to expand the dataset to include more observations and other 
predictor variables to identify if another property of BCG can even more effectively explain BCG mass.

